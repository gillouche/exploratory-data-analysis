{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from num2words import num2words\n",
    "import phonetics\n",
    "import autocorrect\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "from spacy_symspell import SpellingCorrector\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from spacy.util import minibatch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Downloader>  q\n"
     ]
    }
   ],
   "source": [
    "# nltk.download_shell()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text encoding\n",
    "\n",
    "Should probably use UTF-8 for most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text substitution, remove null and empty values\n",
    "\n",
    "Remove noise from raw format -> extract text from html, xml, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'advanced install options & other platforms',\n",
       " 'android',\n",
       " 'automatic privacy is here. download firefox to block over 2000 trackers.',\n",
       " \"avañe'ẽ\",\n",
       " 'bahasa indonesia',\n",
       " 'benefits',\n",
       " 'beta',\n",
       " 'beta for android',\n",
       " 'brand standards',\n",
       " 'browsers',\n",
       " 'cookies',\n",
       " 'cymraeg',\n",
       " 'dansk',\n",
       " 'desktop',\n",
       " 'deutsch',\n",
       " 'developer edition',\n",
       " 'developers',\n",
       " 'dolnoserbšćina',\n",
       " 'download firefox',\n",
       " 'download firefox for linux',\n",
       " 'download firefox for macos',\n",
       " 'download firefox for windows',\n",
       " 'download firefox — free web browser',\n",
       " 'download in another language',\n",
       " 'download now',\n",
       " 'english',\n",
       " 'english (british)',\n",
       " 'english (canadian)',\n",
       " 'enterprise',\n",
       " 'español (de argentina)',\n",
       " 'español (de chile)',\n",
       " 'español (de españa)',\n",
       " 'español (de méxico)',\n",
       " 'euskara',\n",
       " 'firefox',\n",
       " 'firefox lockwise makes the passwords you save in firefox secure and available on all your devices.',\n",
       " 'firefox monitor alerts you if we know your information is a part of another company’s data breach.',\n",
       " 'firefox privacy notice',\n",
       " 'firefox shows you how many data-collecting trackers are blocked with enhanced tracking protection.',\n",
       " 'fix a problem',\n",
       " 'français',\n",
       " 'frysk',\n",
       " 'get more from firefox',\n",
       " 'get the latest firefox browser.',\n",
       " 'go',\n",
       " 'hornjoserbsce',\n",
       " 'hrvatski',\n",
       " 'instagram (@firefox)',\n",
       " 'interlingua',\n",
       " 'ios',\n",
       " 'italiano',\n",
       " 'join',\n",
       " 'just download the browser',\n",
       " 'language',\n",
       " 'legal',\n",
       " 'lietuvių',\n",
       " 'linux 32-bit',\n",
       " 'linux 64-bit',\n",
       " 'lockwise',\n",
       " 'macos',\n",
       " 'magyar',\n",
       " 'make your passwords portable',\n",
       " 'maya kaqchikel',\n",
       " 'mobile',\n",
       " 'monitor',\n",
       " 'mozilla',\n",
       " 'mozilla.org',\n",
       " 'nederlands',\n",
       " 'need help?',\n",
       " 'nightly',\n",
       " 'nightly for android',\n",
       " 'norsk bokmål',\n",
       " 'norsk nynorsk',\n",
       " 'please follow these instructions to install firefox.',\n",
       " 'pocket',\n",
       " 'polski',\n",
       " 'portions of this content are ©1998–2020 by individual mozilla.org contributors. content available under a creative commons license.',\n",
       " 'português (do brasil)',\n",
       " 'português (europeu)',\n",
       " 'press',\n",
       " 'privacy',\n",
       " 'products',\n",
       " 'reality',\n",
       " 'română',\n",
       " 'see what’s being blocked',\n",
       " 'send',\n",
       " 'shqip',\n",
       " 'sign in',\n",
       " 'sign up',\n",
       " 'slovenčina',\n",
       " 'slovenščina',\n",
       " 'svenska',\n",
       " 'taqbaylit',\n",
       " 'tiếng việt',\n",
       " 'twitter (@firefox)',\n",
       " 'türkçe',\n",
       " 'visit mozilla corporation’s not-for-profit parent, the mozilla foundation.',\n",
       " 'watch for data breaches',\n",
       " 'watch for hackers with firefox monitor, protect passwords with firefox lockwise, and more.',\n",
       " 'website privacy notice',\n",
       " 'windows 32-bit',\n",
       " 'windows 32-bit msi',\n",
       " 'windows 64-bit',\n",
       " 'windows 64-bit msi',\n",
       " \"your system doesn't meet the requirements to run firefox.\",\n",
       " 'your system may not meet the requirements for firefox, but you can try one of these versions:',\n",
       " 'youtube (@firefoxchannel)',\n",
       " 'you’ve already got the browser. now get even more from firefox.',\n",
       " 'čeština',\n",
       " 'беларуская',\n",
       " 'русский',\n",
       " 'српски',\n",
       " 'українська',\n",
       " 'मराठी',\n",
       " 'ਪੰਜਾਬੀ (ਭਾਰਤ)',\n",
       " 'ქართული',\n",
       " '— english (us)',\n",
       " '— mozilla',\n",
       " '中文 (简体)',\n",
       " '正體中文 (繁體)'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text_from_markup(markup_text):\n",
    "    soup = BeautifulSoup(markup_text)\n",
    "    lines = soup.text.split('\\n')\n",
    "    tokens = [token.lower().strip() for token in lines if token.strip()]\n",
    "    return set(tokens)\n",
    "\n",
    "\n",
    "with open('data/firefox.html') as g:\n",
    "    html = g.read()\n",
    "\n",
    "extract_text_from_markup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words\n",
    "\n",
    "Commonly occurring words which are not relevant in the context of data.\n",
    "\n",
    "Do not remove for speech tagging or parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"My sentence will not contain stopwords after processing because they will be removed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentence', 'not', 'contain', 'stopwords', 'processing', 'removed.']\n"
     ]
    }
   ],
   "source": [
    "filtered = list()\n",
    "nlp.vocab[\"not\"].is_stop = False\n",
    "nlp.vocab[\"n't\"].is_stop = False\n",
    "for word in sentence.split(' '):\n",
    "    if not nlp.vocab[word].is_stop:\n",
    "        filtered.append(word)\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'sentence', 'contain', 'stopwords', 'processing', 'removed.']\n"
     ]
    }
   ],
   "source": [
    "#nltk\n",
    "\n",
    "filtered = list()\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "for word in sentence.split(' '):\n",
    "    if word not in nltk_stopwords:\n",
    "        filtered.append(word)\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'sentence', 'contain', 'stopwords', 'processing', 'removed.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gensim\n",
    "remove_stopwords(sentence).split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "Split sentences in list of words, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token\tpos\tlemma\tstop_words\n",
      "Tea\tNOUN\ttea\tFalse\n",
      "is\tAUX\tbe\tTrue\n",
      "healthy\tADJ\thealthy\tFalse\n",
      "and\tCCONJ\tand\tTrue\n",
      "calming\tVERB\tcalm\tFalse\n",
      ",\tPUNCT\t,\tFalse\n",
      "do\tAUX\tdo\tTrue\n",
      "n't\tPART\tnot\tTrue\n",
      "you\tPRON\t-PRON-\tTrue\n",
      "think\tVERB\tthink\tFalse\n",
      "?\tPUNCT\t?\tFalse\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "doc = nlp(\"Tea is healthy and calming, don't you think?\")\n",
    "print(f'Token\\tpos\\tlemma\\tstop_words')\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t{token.pos_}\\t{token.lemma_}\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize\n",
    "\n",
    "* change numbers to word equivalent\n",
    "* lowercase everything except for sentiment analysis because we lose information with that\n",
    "* negation handling: aren't -> are not\n",
    "* remove standalone puncuations such as commas, dots, ...\n",
    "* change plurals to singular\n",
    "* lemmatization: find lemma of word (Gensim lib, spacy)\n",
    "* stemming: take the root of the word, can give inaccurate results but faster than lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower().strip()\n",
    "    nlp = spacy.load(\"en\")\n",
    "    \n",
    "    nlp.vocab[\"not\"].is_stop = False\n",
    "    nlp.vocab[\"n't\"].is_stop = False\n",
    "\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    output_token = list()\n",
    "    for token in doc:\n",
    "        print(f\"{token}\\t{token.pos_}\\t{token.lemma_}\\t{token.is_stop}\")\n",
    "        token_str = token.text\n",
    "        if token_str.isdigit():\n",
    "            output_token.append(num2words(token_str))\n",
    "        else:\n",
    "            lemma = token.lemma_\n",
    "            if token.is_punct or token.is_stop:\n",
    "                continue\n",
    "            output_token.append(lemma)\n",
    "    return output_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\tPRON\ti\tTrue\n",
      "am\tAUX\tbe\tTrue\n",
      "a\tDET\ta\tTrue\n",
      "50\tNUM\t50\tFalse\n",
      "years\tNOUN\tyear\tFalse\n",
      "old\tADJ\told\tFalse\n",
      "man\tNOUN\tman\tFalse\n",
      "who\tPRON\twho\tTrue\n",
      "is\tAUX\tbe\tTrue\n",
      "n't\tPART\tnot\tFalse\n",
      "afraid\tADJ\tafraid\tFalse\n",
      "to\tPART\tto\tTrue\n",
      "get\tAUX\tget\tTrue\n",
      "his\tDET\t-PRON-\tTrue\n",
      "hands\tNOUN\thand\tFalse\n",
      "dirty\tADJ\tdirty\tFalse\n",
      "!\tPUNCT\t!\tFalse\n",
      "i\tPRON\ti\tTrue\n",
      "like\tVERB\tlike\tFalse\n",
      "reading\tVERB\tread\tFalse\n",
      "books\tNOUN\tbook\tFalse\n",
      ",\tPUNCT\t,\tFalse\n",
      "learning\tVERB\tlearn\tFalse\n",
      "deep\tADJ\tdeep\tFalse\n",
      "learning\tVERB\tlearn\tFalse\n",
      "with\tADP\twith\tTrue\n",
      "pytorch\tNOUN\tpytorch\tFalse\n",
      "and\tCCONJ\tand\tTrue\n",
      "kaggle\tNOUN\tkaggle\tFalse\n",
      ".\tPUNCT\t.\tFalse\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['fifty',\n",
       " 'year',\n",
       " 'old',\n",
       " 'man',\n",
       " 'not',\n",
       " 'afraid',\n",
       " 'hand',\n",
       " 'dirty',\n",
       " 'like',\n",
       " 'read',\n",
       " 'book',\n",
       " 'learn',\n",
       " 'deep',\n",
       " 'learn',\n",
       " 'pytorch',\n",
       " 'kaggle']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am a 50 years old man who isn't afraid to get his hands dirty! I like reading books, learning Deep Learning with PyTorch and Kaggle.\"\n",
    "normalize_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace synonyms by a same word\n",
    "\n",
    "would that be useful ? This could remove noise from the text but depending on the NLP use case, we would lose information.\n",
    "\n",
    "want, need, required could be classified as strongly similar but \"want\" and \"need\" have different meaning.\n",
    "\n",
    "There doesn't seem to have an easy solution for this so maybe that's not really useful, there is a python lib that finds very similar words using wordnet : https://pypi.org/project/spacy-wordnet/\n",
    "\n",
    "If needed, it should be possible to scrap an online dictionary to extract synonyms for words but we may have weird results. Cambridge dictionary has fuel injection as a synonym for car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonetic hashing\n",
    "\n",
    "* combines the same phonemes (smallest unit of sound) into one bucket and gives them the same hash code for all the variations => colour and color have the same code\n",
    "* technique used to canocalize words that have different variants but same phonetic characteristics, that is, the same pronunciation\n",
    "* Soundexes are algorithm that can be used to calculate the hash code of a given word\n",
    "    * algo differ from language to language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c0406\n",
      "c0406\n"
     ]
    }
   ],
   "source": [
    "print(phonetics.soundex(\"colour\"))\n",
    "print(phonetics.soundex(\"color\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling corrector\n",
    "\n",
    "Nothing really amazing, need to find better solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My text hase eror, I writee lik dat. How r u? Ur an idiot.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(r'[,?\\.]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [regex.sub('', word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'writee', 'hase', 'eror'}\n"
     ]
    }
   ],
   "source": [
    "spell_checker = SpellChecker()\n",
    "misspelled_words = spell_checker.unknown(tokens)\n",
    "print(misspelled_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writee -> write -> {'writhe', 'write', 'writes', 'writer'}\n",
      "hase -> have -> {'haser', 'haase', 'hate', 'hash', 'base', 'hage', 'ease', 'hast', 'haue', 'hose', 'have', 'hause', 'haze', 'hse', 'phase', 'case', 'haste', 'has', 'hale', 'hasp', 'hake', 'hare', 'vase', 'hae', 'ase', 'hanse', 'chase'}\n",
      "eror -> error -> {'eros', 'ebor', 'error', 'emor', 'err', 'egor'}\n"
     ]
    }
   ],
   "source": [
    "for misspelled_word in misspelled_words:\n",
    "    print(f'{misspelled_word} -> {spell_checker.correction(misspelled_word)} -> {spell_checker.candidates(misspelled_word)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hase -> have\n",
      "eror -> error\n",
      "lik -> like\n"
     ]
    }
   ],
   "source": [
    "spell_corrector = autocorrect.Speller(lang=\"en\")\n",
    "for word in tokens:\n",
    "    corrected_word = spell_corrector(word)\n",
    "    if word != corrected_word:\n",
    "        print(f\"{word} -> {corrected_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']\n",
    "patterns = [nlp(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", None, *patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_doc = nlp(\"Glowing review overall, and some really interesting side-by-side \"\n",
    "               \"photography tests pitting the iPhone 11 Pro against the \"\n",
    "               \"Galaxy Note 10 Plus and last year’s iPhone XS and Google Pixel 3.\") \n",
    "matches = matcher(text_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TerminologyList iPhone 11\n",
      "TerminologyList Galaxy Note\n",
      "TerminologyList iPhone XS\n",
      "TerminologyList Google Pixel\n"
     ]
    }
   ],
   "source": [
    "for match in matches:\n",
    "    match_id, start, end = match\n",
    "    print(nlp.vocab.strings[match_id], text_doc[start:end])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "AKA word vectors represent each word by a vector that keeps the info on how the word is used and what it means. Words that appear in similar contexts will have similar vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 96)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam = pd.read_csv('data/spam.csv')\n",
    "\n",
    "with nlp.disable_pipes():\n",
    "    doc_vectors = np.array([nlp(text).vector for text in spam.text])\n",
    "    \n",
    "doc_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(doc_vectors, spam.label,\n",
    "                                                    test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.444%\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC(random_state=1, dual=False, max_iter=10000)\n",
    "svc.fit(X_train, y_train)\n",
    "print(f\"Accuracy: {svc.score(X_test, y_test) * 100:.3f}%\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document similarity\n",
    "\n",
    "* documents with similar content generally have similar vectors\n",
    "* we can find similar documents by measuring the similarity between the vectors.\n",
    "\n",
    "A common metric for this is the cosine simillarity which measures the angle between two vectors, a and b.\n",
    "\n",
    "$cos(\\theta) = \\frac{a * b}{\\vert\\vert a \\vert\\vert * \\vert\\vert b \\vert\\vert}$\n",
    "\n",
    "=> dot product of a and b, divided by the magnitude of each vector. Cosine similarity varies between -1 and 1 corresponding\n",
    "complete opposite to perfect similarity respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7030030981818236"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nlp(\"REPLY NOW FOR FREE TEA\")\n",
    "b = nlp(\"According to legend, Emperor Shen Nung discovered tea when leaves from a wild tree blew into his pot of boiling water.\")\n",
    "a.similarity(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"According to legend, Emperor Shen Nung discovered tea when leaves from a wild tree blew into his pot of boiling water.\"\n",
    "a = nlp(text)\n",
    "b = nlp(text)\n",
    "a.similarity(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3382732593002094"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nlp(\"Alice in wonderland\")\n",
    "b = nlp(\"Python is a nice programming language.\")\n",
    "a.similarity(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy pipelines\n",
    "\n",
    "similar to sklearn pipelines\n",
    "\n",
    "https://spacy.io/usage/processing-pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam = pd.read_csv('data/spam.csv')\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "textcat = nlp.create_pipe(\"textcat\", config={\n",
    "                \"exclusive_classes\": True,\n",
    "                \"architecture\": \"bow\"})\n",
    "\n",
    "nlp.add_pipe(textcat)\n",
    "textcat.add_label(\"ham\")\n",
    "textcat.add_label(\"spam\")\n",
    "train_texts = spam['text'].values\n",
    "train_labels = [{'cats': {'ham': label == 'ham',\n",
    "                          'spam': label == 'spam'}} \n",
    "                for label in spam['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       "  {'cats': {'ham': True, 'spam': False}}),\n",
       " ('Ok lar... Joking wif u oni...', {'cats': {'ham': True, 'spam': False}}),\n",
       " (\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       "  {'cats': {'ham': False, 'spam': True}})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = list(zip(train_texts, train_labels))\n",
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3387270104194613\n",
      "0.8369239149687928\n",
      "0.6208181146248227\n",
      "0.4958138850246252\n",
      "0.4128723095680268\n",
      "0.35273815350844373\n",
      "0.3075636856317985\n",
      "0.27217805334311457\n",
      "0.24401554055971772\n",
      "0.2210127389680426\n",
      "0.2019711623467173\n",
      "0.18582977406817336\n",
      "0.1719950088306258\n",
      "0.16005479902147526\n",
      "0.14967598166822854\n",
      "0.14055339618562634\n",
      "0.13247412087174254\n",
      "0.12527049393425266\n",
      "0.11880373060797\n",
      "0.1133074344318854\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "spacy.util.fix_random_seed(1)\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "losses = {}\n",
    "epoch_count = 1\n",
    "for epoch in range(20):\n",
    "    random.shuffle(train_data)\n",
    "    # Create the batch generator with batch size = 8\n",
    "    batches = minibatch(train_data, size=8)\n",
    "    # Iterate through minibatches\n",
    "    for batch in batches:\n",
    "        # Each batch is a list of (text, label) but we need to\n",
    "        # send separate lists for texts and labels to update().\n",
    "        # This is a quick way to split a list of tuples into lists\n",
    "        texts, labels = zip(*batch)\n",
    "        nlp.update(texts, labels, sgd=optimizer, losses=losses)\n",
    "    print(losses[\"textcat\"] / epoch_count)\n",
    "    epoch_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9999750e-01 2.5330050e-06]\n",
      " [1.5455886e-03 9.9845445e-01]]\n"
     ]
    }
   ],
   "source": [
    "texts = [\"Are you ready for the tea party????? It's gonna be wild\",\n",
    "         \"URGENT Reply to this message for GUARANTEED FREE TEA\" ]\n",
    "docs = [nlp.tokenizer(text) for text in texts]\n",
    "    \n",
    "# Use textcat to get the scores for each doc\n",
    "textcat = nlp.get_pipe('textcat')\n",
    "scores, _ = textcat.predict(docs)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'spam']\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = scores.argmax(axis=1)\n",
    "print([textcat.labels[label] for label in predicted_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('$9.4 million', 'MONEY'), ('the prior year', 'DATE'), ('$2.7 million', 'MONEY')]\n",
      "[('twelve billion dollars', 'MONEY'), ('1b', 'MONEY')]\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
    "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for doc in nlp.pipe(texts, disable=[\"tagger\", \"parser\"]):\n",
    "    # Do something with the doc here\n",
    "    print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-exploratory-data-analysis",
   "language": "python",
   "name": "venv-exploratory-data-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
