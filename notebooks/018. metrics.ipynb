{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification error metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specificity\n",
    "\n",
    "$$\\frac{\\text{TN}}{\\text{TN}+\\text{FP}}$$\n",
    "\n",
    "* number of correct negatives out of the actual negatives\n",
    "* we don't want false positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy \n",
    "\n",
    "$$\\frac{\\text{TP}+\\text{TN}}{\\text{Total}}$$\n",
    "\n",
    "* answers the question: how right am I generally ?\n",
    "* do not use if large class imabalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall / sensitivity / true positive rate (TPR) \n",
    "\n",
    "$$\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$$\n",
    "\n",
    "* metrics to use when false positive are acceptable\n",
    "* the closer the value is to 0, the less FN we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision \n",
    "\n",
    "$$\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$$\n",
    "\n",
    "* proportion of actual positives that were correctly identified\n",
    "* how good am I at identifying positive results ?\n",
    "* we don't want false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score\n",
    "\n",
    "$$\\frac{2}{ \\frac{1}{ \\text{Recall} } + \\frac{1}{\\text{Precision}} } = \\frac{ \\text{Recall} * \\text{Precision} }{ \\text{Recall} + \\text{Precision} } * 2$$\n",
    "\n",
    "* combination of recall and precision -> take into account FP and FN\n",
    "* should be used when uneven class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area under the receiver operating characteristic curve (ROC/AUC)\n",
    "\n",
    "gives value between 0.5 (total random prediction) and 1 (best)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall curve (PRC)\n",
    "\n",
    "* more informative than the ROC curve when evaluating binary classifiers on **unbalanced** datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression error metrics\n",
    "\n",
    "The values we get need to be put into context with the values we are trying to predict. If we try to predict prices and we are off by 4 euros, this error result can be good if we are supposed to predict high price with high variance min-max (700 euros mean) or terrible if we want to predict low price (1 euro mean)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean absolute error\n",
    "\n",
    "The mean absolute error uses the same scale as the data being measured so we can't use it for comparisons between series using different scales.\n",
    "\n",
    "Commonly used for forecast error in time series analysis\n",
    "\n",
    "**does not punish large errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean squared error\n",
    "\n",
    "average of the squares of the errors (average squared difference between the estimated values and the actual values). \n",
    "\n",
    "This value is always non negative and the closer we are to 0, the better.\n",
    "\n",
    "**punish large errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean square error\n",
    "\n",
    "The smaller the value of the RMSE, the better the predictive accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on the scores\n",
    "\n",
    "sklearn.metrics.explained_variance_score gives us information on our predictions against ground truth to try to figure out how we can improve.\n",
    "\n",
    "Best possible score is 1.0, less is worse.\n",
    "\n",
    "To also try to make sense of our results, we can plot y_test against y_test in a scatter plot that gives us the best possible predictions as a line and y_test against predictions in the scatter plots to see the difference. We could see info such as outliers that could give us problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfit / overfit\n",
    "\n",
    "* overfit: learned the noise from the train data and not the general pattern => too many features, need more regularization, ... (high variance)\n",
    "* underfit: doesn't learn anything from the data => model not complex enough, not enough data, ... (high bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-exploratory-data-analysis",
   "language": "python",
   "name": "venv-exploratory-data-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
